# -*- coding: utf-8 -*-
"""project_744.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tg2K_06s5k5A6TnxDKZLMSMribk841mA
"""

!pip install pycountry

# Commented out IPython magic to ensure Python compatibility.
# Import fundamentals
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import re
import pycountry
import pickle

# Import nltk and download punkt, wordnet
import nltk


nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')


# Import word_tokenize and stopwords from nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer 
# Gensim
from gensim.models import Word2Vec

# Import the TextBlob
from textblob import TextBlob

# Sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score

# Import wordcloud
from wordcloud import WordCloud

# I will keep the resulting plots
# %matplotlib inline

# Enable Jupyter Notebook's intellisense
# %config IPCompleter.greedy=True

# We want to see whole content (non-truncated)
pd.set_option('display.max_colwidth', None)

# Load the tweets
tweets_raw = pd.read_csv('/content/tweets_raw.csv')

# Print the first five rows
display(tweets_raw.head())

# Print the summary statistics
print(tweets_raw.describe())

# Print the info
print(tweets_raw.info())

tweets_raw = tweets_raw.rename(columns={'user_name': 'Username'})
tweets_raw = tweets_raw.rename(columns={'user_location': 'Location'})
tweets_raw = tweets_raw.rename(columns={'date': 'Created at'})
tweets_raw = tweets_raw.rename(columns={'text': 'Content'})
tweets_raw = tweets_raw.rename(columns={'retweets': 'Retweet-Count'})
tweets_raw = tweets_raw.rename(columns={'favorites': 'Favorites'})

# We do not need first two columns. Let's drop them out.
tweets_raw.drop(columns=["user_description", "user_created", "user_followers", "user_friends",  "user_favourites", "user_verified", "hashtags", "source", "is_retweet" ], axis=1, inplace=True)

print(tweets_raw.info())

# Drop duplicated rows
tweets_raw.drop_duplicates(inplace=True)

# Created at column's type should be datatime
tweets_raw["Created at"] = pd.to_datetime(tweets_raw["Created at"])

# Print the info again
print(tweets_raw.info())

#clean text

#clean function
def cleanTxt(text):
  text = re.sub(r'https?:\/\/\S+', '', text) # remove links
  text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove @mention
  text = re.sub(r'#', '', text) # remove # hashtags
  text = re.sub(r'RT[\s]+', '', text) # remove RT retweet
  return text

#aplly function in our tweets
tweets_raw["Content"]= tweets_raw["Content"].apply(cleanTxt)

#show tweets after cleaninig
tweets_raw

def process_tweets(tweet):
    
    # Remove links
    tweet = re.sub(r"http\S+|www\S+|https\S+", '', tweet, flags=re.MULTILINE)
    
    # Remove mentions and hashtag
    tweet = re.sub(r'\@\w+|\#','', tweet)
    
    # Tokenize the words
    tokenized = word_tokenize(tweet)

    # Remove the stop words
    tokenized = [token for token in tokenized if token not in stopwords.words("english")] 

    # Lemmatize the words
    lemmatizer = WordNetLemmatizer()
    tokenized = [lemmatizer.lemmatize(token, pos='a') for token in tokenized]

    # Remove non-alphabetic characters and keep the words contains three or more letters
    tokenized = [token for token in tokenized if token.isalpha() and len(token)>2]
    
    return tokenized
    
# Call the function and store the result into a new column
tweets_raw["Processed"] = tweets_raw["Content"].str.lower().apply(process_tweets)

# Print the first fifteen rows of Processed
display(tweets_raw[["Processed"]].head(15))

# Get the tweet lengths
tweets_raw["Length"] = tweets_raw["Content"].str.len()

# Get the number of words in tweets
tweets_raw["Words"] = tweets_raw["Content"].str.split().str.len()

# Display the new columns
display(tweets_raw[["Length", "Words"]])

# Fill the missing values with unknown tag
tweets_raw["Location"].fillna("unknown", inplace=True)

# Print the unique locations and number of unique locations
print("Unique Values:",tweets_raw["Location"].unique())
print("Unique Value count:",len(tweets_raw["Location"].unique()))

def get_countries(location):
    
    # If location is a country name return its alpha2 code
    if pycountry.countries.get(name= location):
        return pycountry.countries.get(name = location).alpha_2
    
    # If location is a subdivisions name return the countries alpha2 code
    try:
        pycountry.subdivisions.lookup(location)
        return pycountry.subdivisions.lookup(location).country_code
    except:
        # If the location is neither country nor subdivision return the "unknown" tag
        return "unknown"

# Call the function and store the country codes in the Country column
tweets_raw["Country"] = tweets_raw["Location"].apply(get_countries)

# Print the unique values
print(tweets_raw["Country"].unique())

# Print the number of unique values
print("Number of unique values:",len(tweets_raw["Country"].unique()))

tweets_raw["Processed"]

# Print the minimum datetime
print("Since:",tweets_raw["Created at"].min())
# Print the maximum datetime
print("Until",tweets_raw["Created at"].max())

# Set the seaborn style
sns.set()
# Plot the histogram of hours
sns.distplot(tweets_raw["Created at"].dt.hour, bins=24)
plt.title("Hourly Distribution of Tweets")
plt.show()

"""#The histogram demonstrates that most of the tweets are created between 13 pm-19 pm in a day. The most popular hour is about 17 pm."""

# Print the value counts of Country column
print(tweets_raw["Country"].value_counts())

"""#Apparently, the locations will be noninformative for us because we have 6585 unknown locations. But we can still check the top tweeting countries."""

# We need to exclude unknowns
countries = tweets_raw[tweets_raw.Country!='unknown']

# Select the top 20 countries
top_countries = countries["Country"].value_counts(sort=True).head(20)

# Convert alpha2 country codes to country names and store in a list
country_fullnames = []
for alpha2 in top_countries.index:
    country_fullnames.append(pycountry.countries.get(alpha_2=alpha2).name)

# Visualize the top 20 countries
plt.figure(figsize=(15,10))
sns.barplot(x=country_fullnames,y=top_countries)
plt.xlabel("Countries")
plt.ylabel("Tweet count")
plt.title("Top 20 Countries")
plt.xticks(rotation=90)
plt.show()

"""#According to the bar plot above, United Kingdom, Malaysia, India, United States and Canada,are the top 5 countries in our dataset."""

# Display the most popular tweets which has highest Favorites and Retweet
display(tweets_raw.sort_values(by=["Favorites","Retweet-Count", ], axis=0, ascending=False)[["Content","Retweet-Count","Favorites"]].head(20))

# fucntion to get subjectivity
def getSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

#function to get polarity
def getPolarity(text):
  return TextBlob(text).sentiment.polarity

#create new columns
tweets_raw['subjectivity']= tweets_raw["Content"].apply(getSubjectivity)
tweets_raw['polarity']= tweets_raw["Content"].apply(getPolarity)

#show new df
tweets_raw

#function to compute neg, neu, pos analysis,, Subjectivity score opinion
def getAnalysis(score):
  if score < 0:
    return'Negative'
  elif score == 0:
    return 'Neutral'
  else:
    return 'Positive'

def getAnalysisSubjectivity(score):
  if score < 0.5:
    return'Objective'
  else:
    return 'Subjective'

tweets_raw['Subjectivity_lable'] = tweets_raw['subjectivity'].apply(getAnalysisSubjectivity)
tweets_raw['Polarity_lable'] = tweets_raw['polarity'].apply(getAnalysis)
#show new df
tweets_raw

# Print the value counts of the Label column
print(tweets_raw['Polarity_lable'].value_counts())

# Print the value counts of the Label column
print(tweets_raw['Subjectivity_lable'].value_counts())

# Change the datatype as "category"
tweets_raw["Polarity_lable"] = tweets_raw["Polarity_lable"].astype("category")
# Visualize the Label counts
sns.countplot(tweets_raw["Polarity_lable"])
plt.title("Label Counts")
plt.show()
# Visualize the Polarity scores
plt.figure(figsize = (10, 10)) 
sns.scatterplot(x="polarity", y="subjectivity", hue="Polarity_lable", data=tweets_raw)
plt.title("Subjectivity vs Polarity")
plt.show()

# Display the most positive tweets and compare with Favorites and Retweet to test how good it is the result we got from sentiment analysis 
display(tweets_raw.sort_values(by=["polarity", "Retweet-Count", "Favorites"], axis=0, ascending=False)[["Content","Retweet-Count","Favorites","polarity"]].head(20))
# Display the negative tweets
display(tweets_raw.sort_values(by=["polarity", "Retweet-Count", "Favorites"], axis=0, ascending=[True, False, False])[["Content","Retweet-Count","Favorites","polarity"]].head(20))

# Get the positive/negative counts by country
positives_by_country = tweets_raw[tweets_raw.Country!='unknown'].groupby("Polarity_lable")["Country"].value_counts().Negative.sort_values(ascending=False)
negatives_by_country =tweets_raw[tweets_raw.Country!='unknown'].groupby("Polarity_lable")["Country"].value_counts().Positive.sort_values(ascending=False)

# Print them out
print("Positive \n")
print(positives_by_country)
print("\nNegative\n")
print(negatives_by_country)

# Create a mask for top 1 countries (by tweets count)
mask = tweets_raw["Country"].isin(top_countries.index[:10]).values

# Create a new DataFrame only includes top10 country
top_20df = tweets_raw.iloc[mask,:]

# Visualize the top 20 countries
plt.figure(figsize=(12,10))
sns.countplot(x="Country", hue="Polarity_lable", data=top_20df, order=top_20df["Country"].value_counts().index)
plt.xlabel("Countries")
locs, labels = plt.xticks()
plt.xticks(locs, country_fullnames[:10])
plt.xticks(rotation=45)
plt.ylabel("Tweet count")
plt.title("Top 10 Countries")
plt.show()

#Is there any relationship between the time and tweets’ polarities?
positive = tweets_raw.loc[tweets_raw.Polarity_lable=="Positive"]["Created at"].dt.hour
negative = tweets_raw.loc[tweets_raw.Polarity_lable=="Negative"]["Created at"].dt.hour
#neutral = tweets_raw.loc[tweets_raw.Polarity_lable=="Neutral"]["Created at"].dt.hour
plt.hist(positive, alpha=0.5, bins=24, label="Positive", density=True)
plt.hist(negative, alpha=0.5, bins=24, label="Negative", density=True)
#plt.hist(neutral, alpha=0.5, bins=24, label="Neutral", density=True)
plt.xlabel("Hour")
plt.ylabel("PDF")
plt.title("Hourly Distribution of Tweets")
plt.legend(loc='upper right')
plt.show()

"""The histogram above demonstrates that there is no relationship between the time and tweets’ polarities."""